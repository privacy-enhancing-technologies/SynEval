#!/usr/bin/env python3

import argparse
import pandas as pd
import json
import numpy as np
from pathlib import Path
from typing import Dict, List
from fidelity import FidelityEvaluator
from utility import UtilityEvaluator
from diversity import DiversityEvaluator
from privacy import PrivacyEvaluator
import logging


class SingleDatasetEvaluator:
    def __init__(self, data: pd.DataFrame, metadata: Dict):
        """
        å•æ•°æ®é›†è¯„ä¼°å™¨ï¼Œç”¨äºç­›é€‰åˆé€‚çš„è®­ç»ƒé›†
        
        Args:
            data: è¦è¯„ä¼°çš„æ•°æ®é›†
            metadata: æ•°æ®é›†çš„å…ƒæ•°æ®ä¿¡æ¯
        """
        self.data = data
        self.metadata = metadata
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨ä¸º None - éœ€è¦æ—¶åˆ›å»º
        self._fidelity_evaluator = None
        self._diversity_evaluator = None
        self._privacy_evaluator = None
        self._utility_evaluator = None

    def evaluate_fidelity(self, selected_metrics: List[str] = None) -> Dict:
        """è¯„ä¼°æ•°æ®ä¿çœŸåº¦ï¼ˆæ•°æ®è´¨é‡ï¼‰"""
        if self._fidelity_evaluator is None:
            # å¯¹äºå•æ•°æ®é›†è¯„ä¼°ï¼Œä½¿ç”¨ç›¸åŒçš„æ•°æ®ä½œä¸ºåŸå§‹æ•°æ®
            self._fidelity_evaluator = FidelityEvaluator(self.data, self.data, self.metadata, selected_metrics)
        else:
            # æ›´æ–°é€‰å®šçš„æŒ‡æ ‡
            self._fidelity_evaluator.selected_metrics = selected_metrics if selected_metrics else self._fidelity_evaluator.available_metrics
        return self._fidelity_evaluator.evaluate()

    def evaluate_utility(self, input_columns: List[str], output_columns: List[str], selected_metrics: List[str] = None) -> Dict:
        """è¯„ä¼°æ•°æ®å®ç”¨æ€§ï¼ˆæœºå™¨å­¦ä¹ ä»»åŠ¡æ€§èƒ½ï¼‰"""
        # æ¯æ¬¡åˆ›å»ºæ–°çš„å®ç”¨è¯„ä¼°å™¨ï¼Œå› ä¸ºå®ƒéœ€è¦è¾“å…¥/è¾“å‡ºåˆ—
        utility_evaluator = UtilityEvaluator(
            self.data,
            self.data,  # ä½¿ç”¨ç›¸åŒæ•°æ®ä½œä¸ºåŸå§‹æ•°æ®
            self.metadata,
            input_columns=input_columns,
            output_columns=output_columns,
            selected_metrics=selected_metrics
        )
        return utility_evaluator.evaluate()

    def evaluate_diversity(self, selected_metrics: List[str] = None) -> Dict:
        """è¯„ä¼°æ•°æ®å¤šæ ·æ€§"""
        if self._diversity_evaluator is None:
            self._diversity_evaluator = DiversityEvaluator(self.data, self.data, self.metadata, selected_metrics=selected_metrics)
        else:
            # æ›´æ–°é€‰å®šçš„æŒ‡æ ‡
            self._diversity_evaluator.selected_metrics = selected_metrics if selected_metrics else self._diversity_evaluator.available_metrics
        return self._diversity_evaluator.evaluate()

    def evaluate_privacy(self, selected_metrics: List[str] = None) -> Dict:
        """è¯„ä¼°æ•°æ®éšç§æ€§"""
        if self._privacy_evaluator is None:
            self._privacy_evaluator = PrivacyEvaluator(self.data, self.data, self.metadata, selected_metrics)
        else:
            # æ›´æ–°é€‰å®šçš„æŒ‡æ ‡
            self._privacy_evaluator.selected_metrics = selected_metrics if selected_metrics else self._privacy_evaluator.available_metrics
        return self._privacy_evaluator.evaluate()

    def evaluate_correlation(self, correlation_types, **kwargs):
        """è¯„ä¼°è·¨æ¨¡æ€ç›¸å…³æ€§"""
        if self._utility_evaluator is None:
            self._utility_evaluator = UtilityEvaluator(self.data, self.data, self.metadata, input_columns=[], output_columns=[])
        return self._utility_evaluator.evaluate_correlation(correlation_types, **kwargs)

    def get_dataset_summary(self) -> Dict:
        """è·å–æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        summary = {
            'dataset_info': {
                'shape': self.data.shape,
                'columns': list(self.data.columns),
                'memory_usage_mb': self.data.memory_usage(deep=True).sum() / 1024 / 1024,
                'missing_values': self.data.isnull().sum().to_dict(),
                'duplicate_rows': self.data.duplicated().sum()
            },
            'column_types': {},
            'basic_statistics': {}
        }
        
        # æŒ‰åˆ—ç±»å‹åˆ†ç»„ç»Ÿè®¡
        for col, info in self.metadata.get('columns', {}).items():
            if col in self.data.columns:
                col_type = info.get('sdtype', 'unknown')
                summary['column_types'][col] = col_type
                
                if col_type == 'numerical':
                    summary['basic_statistics'][col] = {
                        'mean': self.data[col].mean(),
                        'std': self.data[col].std(),
                        'min': self.data[col].min(),
                        'max': self.data[col].max(),
                        'median': self.data[col].median()
                    }
                elif col_type == 'categorical':
                    summary['basic_statistics'][col] = {
                        'unique_values': self.data[col].nunique(),
                        'most_common': self.data[col].value_counts().head(5).to_dict()
                    }
                elif col_type == 'text':
                    text_lengths = self.data[col].str.len()
                    summary['basic_statistics'][col] = {
                        'avg_length': text_lengths.mean(),
                        'min_length': text_lengths.min(),
                        'max_length': text_lengths.max(),
                        'empty_texts': (self.data[col] == '').sum()
                    }
        
        return summary


def parse_args():
    parser = argparse.ArgumentParser(description='å•æ•°æ®é›†è¯„ä¼°æ¡†æ¶ - ç”¨äºç­›é€‰åˆé€‚çš„è®­ç»ƒé›†')

    parser.add_argument('--data', required=True, help='è¦è¯„ä¼°çš„æ•°æ®é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--metadata', required=True, help='å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„')

    parser.add_argument('--dimensions', nargs='+', 
                        choices=['fidelity', 'utility', 'diversity', 'privacy', 'summary'],
                        default=['summary', 'fidelity', 'diversity', 'privacy'],
                        help='è¦è¿è¡Œçš„è¯„ä¼°ç»´åº¦ (é»˜è®¤: summary, fidelity, diversity, privacy)')
    parser.add_argument('--dimension', nargs='+', 
                        choices=['fidelity', 'utility', 'diversity', 'privacy', 'summary'],
                        help='--dimensions çš„åˆ«å (å•æ•°å½¢å¼)')

    parser.add_argument('--utility-input', nargs='+', help='å®ç”¨è¯„ä¼°çš„è¾“å…¥åˆ—')
    parser.add_argument('--utility-output', nargs='+', help='å®ç”¨è¯„ä¼°çš„è¾“å‡ºåˆ—')

    # ä¿çœŸåº¦æŒ‡æ ‡é€‰æ‹©
    parser.add_argument('--fidelity-metrics', nargs='+',
                        choices=['diagnostic', 'quality', 'text', 'numerical_statistics'],
                        help='è¦è¿è¡Œçš„ç‰¹å®šä¿çœŸåº¦æŒ‡æ ‡ (é»˜è®¤: å…¨éƒ¨)')
    
    # å¤šæ ·æ€§æŒ‡æ ‡é€‰æ‹©
    parser.add_argument('--diversity-metrics', nargs='+',
                        choices=['tabular_diversity', 'text_diversity'],
                        help='è¦è¿è¡Œçš„ç‰¹å®šå¤šæ ·æ€§æŒ‡æ ‡ (é»˜è®¤: å…¨éƒ¨)')
    
    # å®ç”¨æ€§æŒ‡æ ‡é€‰æ‹©
    parser.add_argument('--utility-metrics', nargs='+',
                        choices=['tstr_accuracy', 'correlation_analysis'],
                        help='è¦è¿è¡Œçš„ç‰¹å®šå®ç”¨æ€§æŒ‡æ ‡ (é»˜è®¤: å…¨éƒ¨)')
    
    # éšç§æŒ‡æ ‡é€‰æ‹©
    parser.add_argument('--privacy-metrics', nargs='+',
                        choices=['exact_matches', 'membership_inference', 'tabular_privacy', 'text_privacy', 'anonymeter'],
                        help='è¦è¿è¡Œçš„ç‰¹å®šéšç§æŒ‡æ ‡ (é»˜è®¤: å…¨éƒ¨)')
    
    # ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆå‘åå…¼å®¹ï¼‰
    parser.add_argument('--correlation', nargs='+',
                        choices=['sentiment_rating', 'keyword_category', 'numeric_length', 'semantic_tabular', 'pii_text_leakage'],
                        help='è¦è¿è¡Œçš„è·¨æ¨¡æ€ç›¸å…³æ€§æŒ‡æ ‡ (é—ç•™ï¼Œä½¿ç”¨ --utility-metrics correlation_analysis)')
    parser.add_argument('--correlation-text-col', default='review', help='ç›¸å…³æ€§åˆ†æçš„æ–‡æœ¬åˆ—')
    parser.add_argument('--correlation-rating-col', default='rating', help='æƒ…æ„Ÿç›¸å…³æ€§çš„è¯„åˆ†åˆ—')
    parser.add_argument('--correlation-category-col', default='category', help='å…³é”®è¯ç›¸å…³æ€§çš„ç±»åˆ«åˆ—')
    parser.add_argument('--correlation-numeric-col', default='price', help='é•¿åº¦ç›¸å…³æ€§çš„æ•°å€¼åˆ—')
    parser.add_argument('--correlation-pii-col', default='user_id', help='æ³„æ¼ç›¸å…³æ€§çš„PIIåˆ—')
    parser.add_argument('--correlation-top-n', type=int, default=50, help='å…³é”®è¯-ç±»åˆ«ç›¸å…³æ€§çš„å‰Nä¸ªå…³é”®è¯')

    parser.add_argument('--output', default='./single_dataset_evaluation.json', help='ä¿å­˜è¯„ä¼°ç»“æœçš„è·¯å¾„')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], help='è®¾ç½®æ—¥å¿—çº§åˆ«')
    parser.add_argument('--plot', action='store_true', help='ä¸ºæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ç”Ÿæˆå›¾è¡¨å¹¶ä¿å­˜åˆ° ./plots ç›®å½•')
    
    return parser

def main():
    parser = parse_args()
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # åŠ è½½æ•°æ®
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.data}")
    data = pd.read_csv(args.data)

    logger.info(f"æ­£åœ¨åŠ è½½å…ƒæ•°æ®: {args.metadata}")
    with open(args.metadata, 'r') as f:
        metadata = json.load(f)

    # åˆ é™¤æœªåœ¨å…ƒæ•°æ®ä¸­å®šä¹‰çš„ _id
    metadata_columns = metadata.get("columns", {})
    if '_id' in data.columns and '_id' not in metadata_columns:
        logger.warning("åœ¨æ•°æ®ä¸­å‘ç° '_id' åˆ—ä½†å…ƒæ•°æ®ä¸­æœªå®šä¹‰ â€” æ­£åœ¨åˆ é™¤")
        data.drop(columns=['_id'], inplace=True)

    # å¤„ç† --dimensions å’Œ --dimension å‚æ•°
    dimensions = args.dimensions
    if args.dimension:
        dimensions = args.dimension
    
    # éªŒè¯å®ç”¨å‚æ•°ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if 'utility' in dimensions:
        if not args.utility_input or not args.utility_output:
            parser.error("å®ç”¨è¯„ä¼°éœ€è¦ --utility-input å’Œ --utility-output å‚æ•°")

    # è¿è¡Œè¯„ä¼°
    evaluator = SingleDatasetEvaluator(data, metadata)
    results = {}

    if 'summary' in dimensions:
        logger.info("æ­£åœ¨ç”Ÿæˆæ•°æ®é›†æ‘˜è¦...")
        results['summary'] = evaluator.get_dataset_summary()

    if 'fidelity' in dimensions:
        logger.info("æ­£åœ¨è¿è¡Œä¿çœŸåº¦è¯„ä¼°...")
        results['fidelity'] = evaluator.evaluate_fidelity(selected_metrics=args.fidelity_metrics)

    if 'utility' in dimensions:
        logger.info("æ­£åœ¨è¿è¡Œå®ç”¨æ€§è¯„ä¼°...")
        results['utility'] = evaluator.evaluate_utility(
            input_columns=args.utility_input,
            output_columns=args.utility_output,
            selected_metrics=args.utility_metrics
        )

    if 'diversity' in dimensions:
        logger.info("æ­£åœ¨è¿è¡Œå¤šæ ·æ€§è¯„ä¼°...")
        try:
            results['diversity'] = evaluator.evaluate_diversity(selected_metrics=args.diversity_metrics)
            logger.info("å¤šæ ·æ€§è¯„ä¼°æˆåŠŸå®Œæˆ")
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§è¯„ä¼°å‡ºé”™: {str(e)}")
            results['diversity'] = {
                'error': str(e),
                'tabular_diversity': {},
                'text_diversity': {}
            }

    if 'privacy' in dimensions:
        logger.info("æ­£åœ¨è¿è¡Œéšç§è¯„ä¼°...")
        try:
            results['privacy'] = evaluator.evaluate_privacy(selected_metrics=args.privacy_metrics)
            logger.info("éšç§è¯„ä¼°æˆåŠŸå®Œæˆ")
        except Exception as e:
            logger.error(f"éšç§è¯„ä¼°å‡ºé”™: {str(e)}")
            results['privacy'] = {
                'error': str(e),
                'membership_inference': {},
                'exact_matches': {}
            }

    if args.correlation:
        logger.info(f"æ­£åœ¨è¿è¡Œç›¸å…³æ€§è¯„ä¼°: {args.correlation}")
        correlation_kwargs = dict(
            text_col=args.correlation_text_col,
            rating_col=args.correlation_rating_col,
            category_col=args.correlation_category_col,
            numeric_col=args.correlation_numeric_col,
            pii_col=args.correlation_pii_col,
            top_n=args.correlation_top_n
        )
        results['correlation'] = evaluator.evaluate_correlation(args.correlation, **correlation_kwargs)

    # åœ¨ä¿å­˜å‰è½¬æ¢ä»»ä½•ä¸å¯åºåˆ—åŒ–çš„å€¼ï¼ˆä¾‹å¦‚ NaNï¼‰
    def safe_json(obj):
        try:
            return json.loads(json.dumps(obj, default=str))
        except Exception as e:
            logger.error(f"åºåˆ—åŒ–ç»“æœæ—¶å‡ºé”™: {str(e)}")
            return {"error": "åºåˆ—åŒ–ç»“æœå¤±è´¥", "original_error": str(e)}

    # ä¿å­˜ç»“æœ
    logger.info(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ° {args.output}")
    try:
        safe_results = safe_json(results)
        with open(args.output, 'w') as f:
            json.dump(safe_results, f, indent=2)
        logger.info(f"ç»“æœå·²æˆåŠŸä¿å­˜åˆ° {args.output}")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœåˆ° {args.output} æ—¶å‡ºé”™: {str(e)}")
        # å°è¯•ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶
        backup_file = f"{args.output}.backup"
        try:
            with open(backup_file, 'w') as f:
                json.dump(safe_results, f, indent=2)
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶: {backup_file}")
        except Exception as backup_e:
            logger.error(f"ä¿å­˜å¤‡ä»½æ–‡ä»¶å¤±è´¥: {str(backup_e)}")

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\nğŸ“Š å•æ•°æ®é›†è¯„ä¼°ç»“æœæ‘˜è¦")
    print("=" * 50)
    display_results_summary(results)

    # å¦‚æœè¯·æ±‚åˆ™ç”Ÿæˆå›¾è¡¨
    if getattr(args, 'plot', False):
        logger.info("æ­£åœ¨ä¸ºè¯„ä¼°ç»“æœç”Ÿæˆå›¾è¡¨...")
        try:
            from plotting import SynEvalPlotter
            plotter = SynEvalPlotter(output_dir='./plots')
            plotter.plot_all_results(results, save_plots=True)
            logger.info("å›¾è¡¨å·²ä¿å­˜åˆ° ./plots ç›®å½•")
        except ImportError as e:
            logger.warning(f"ç»˜å›¾æ¨¡å—ä¸å¯ç”¨: {str(e)}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {str(e)}")

    logger.info("è¯„ä¼°å®Œæˆ!")
    return results

def display_results_summary(results: Dict):
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœæ‘˜è¦"""
    
    for dimension, dimension_results in results.items():
        print(f"\nğŸ” {dimension.upper()} è¯„ä¼°")
        print("-" * 30)
        
        if dimension == 'summary':
            if 'dataset_info' in dimension_results:
                info = dimension_results['dataset_info']
                print(f"æ•°æ®é›†å½¢çŠ¶: {info.get('shape', 'N/A')}")
                print(f"å†…å­˜ä½¿ç”¨: {info.get('memory_usage_mb', 'N/A'):.2f} MB")
                print(f"é‡å¤è¡Œæ•°: {info.get('duplicate_rows', 'N/A')}")
                print(f"åˆ—æ•°: {len(info.get('columns', []))}")
                
                missing_values = info.get('missing_values', {})
                total_missing = sum(missing_values.values())
                if total_missing > 0:
                    print(f"æ€»ç¼ºå¤±å€¼: {total_missing}")
                    # æ˜¾ç¤ºç¼ºå¤±å€¼æœ€å¤šçš„å‰3åˆ—
                    sorted_missing = sorted(missing_values.items(), key=lambda x: x[1], reverse=True)[:3]
                    for col, count in sorted_missing:
                        if count > 0:
                            print(f"  {col}: {count}")
        
        elif dimension == 'fidelity':
            if 'diagnostic' in dimension_results and dimension_results['diagnostic']:
                diag = dimension_results['diagnostic']
                print(f"æ•°æ®æœ‰æ•ˆæ€§: {diag.get('Data Validity', 'N/A')}")
                print(f"æ•°æ®ç»“æ„: {diag.get('Data Structure', 'N/A')}")
                print(f"æ€»ä½“è¯„åˆ†: {diag.get('Overall', {}).get('score', 'N/A')}")
            
            if 'numerical_statistics' in dimension_results:
                num_stats = dimension_results['numerical_statistics']
                if num_stats:
                    fidelity_scores = [col_data.get('overall_fidelity_score', 0) 
                                     for col_data in num_stats.values() 
                                     if isinstance(col_data, dict) and 'overall_fidelity_score' in col_data]
                    if fidelity_scores:
                        avg_fidelity = sum(fidelity_scores) / len(fidelity_scores)
                        if isinstance(avg_fidelity, (int, float)):
                            print(f"å¹³å‡æ•°å€¼ä¿çœŸåº¦: {avg_fidelity:.3f}")
                        else:
                            print(f"å¹³å‡æ•°å€¼ä¿çœŸåº¦: {avg_fidelity}")
        
        elif dimension == 'utility':
            if 'tstr_accuracy' in dimension_results:
                tstr_results = dimension_results['tstr_accuracy']
                if 'real_data_model' in tstr_results and 'synthetic_data_model' in tstr_results:
                    real_acc = tstr_results['real_data_model'].get('accuracy', 0)
                    syn_acc = tstr_results['synthetic_data_model'].get('accuracy', 0)
                    
                    real_acc_formatted = f"{real_acc:.3f}" if isinstance(real_acc, (int, float)) else str(real_acc)
                    syn_acc_formatted = f"{syn_acc:.3f}" if isinstance(syn_acc, (int, float)) else str(syn_acc)
                    
                    print(f"ä»»åŠ¡ç±»å‹: {tstr_results.get('task_type', 'N/A')}")
                    print(f"è®­ç»ƒå¤§å°: {tstr_results.get('training_size', 'N/A')}")
                    print(f"æµ‹è¯•å¤§å°: {tstr_results.get('test_size', 'N/A')}")
                    print(f"æ¨¡å‹å‡†ç¡®ç‡: {syn_acc_formatted}")
        
        elif dimension == 'diversity':
            if 'tabular_diversity' in dimension_results:
                tab = dimension_results['tabular_diversity']
                
                if 'coverage' in tab and tab['coverage']:
                    avg_coverage = sum(tab['coverage'].values()) / len(tab['coverage'])
                    if isinstance(avg_coverage, (int, float)):
                        print(f"å¹³å‡è¡¨æ ¼è¦†ç›–ç‡: {avg_coverage:.1f}%")
                    else:
                        print(f"å¹³å‡è¡¨æ ¼è¦†ç›–ç‡: {avg_coverage}%")
                
                if 'uniqueness' in tab and tab['uniqueness']:
                    if 'duplicate_ratio' in tab['uniqueness']:
                        dup_ratio = tab['uniqueness']['duplicate_ratio']
                        if isinstance(dup_ratio, (int, float)):
                            print(f"é‡å¤æ¯”ä¾‹: {dup_ratio:.3f}")
                        else:
                            print(f"é‡å¤æ¯”ä¾‹: {dup_ratio}")
                
                if 'entropy_metrics' in tab and tab['entropy_metrics']:
                    if 'dataset_entropy' in tab['entropy_metrics']:
                        entropy_data = tab['entropy_metrics']['dataset_entropy']
                        if 'entropy_ratio' in entropy_data:
                            entropy_ratio = entropy_data['entropy_ratio']
                            if isinstance(entropy_ratio, (int, float)):
                                print(f"ç†µæ¯”ä¾‹: {entropy_ratio:.3f}")
                            else:
                                print(f"ç†µæ¯”ä¾‹: {entropy_ratio}")
            
            if 'text_diversity' in dimension_results:
                text_div = dimension_results['text_diversity']
                if 'synthetic' in text_div and text_div['synthetic']:
                    first_col = next(iter(text_div['synthetic']), None)
                    if first_col:
                        col_results = text_div['synthetic'][first_col]
                        
                        if 'lexical_diversity' in col_results:
                            lex_div = col_results['lexical_diversity']
                            if '1-gram' in lex_div and 'unique_ratio' in lex_div['1-gram']:
                                unique_ratio = lex_div['1-gram']['unique_ratio']
                                if isinstance(unique_ratio, (int, float)):
                                    print(f"æ–‡æœ¬è¯æ±‡å¤šæ ·æ€§: {unique_ratio:.3f}")
                                else:
                                    print(f"æ–‡æœ¬è¯æ±‡å¤šæ ·æ€§: {unique_ratio}")
                        
                        if 'semantic_diversity' in col_results:
                            sem_div = col_results['semantic_diversity']
                            if 'distinct_ratio' in sem_div:
                                distinct_ratio = sem_div['distinct_ratio']
                                if isinstance(distinct_ratio, (int, float)):
                                    print(f"æ–‡æœ¬è¯­ä¹‰å¤šæ ·æ€§: {distinct_ratio:.3f}")
                                else:
                                    print(f"æ–‡æœ¬è¯­ä¹‰å¤šæ ·æ€§: {distinct_ratio}")
        
        elif dimension == 'privacy':
            if 'membership_inference' in dimension_results:
                mia = dimension_results['membership_inference']
                print(f"æˆå‘˜æ¨ç†é£é™©: {mia.get('risk_level', 'N/A')}")
                
                mia_auc = mia.get('mia_auc_score', None)
                if isinstance(mia_auc, (int, float)):
                    print(f"MIA AUC è¯„åˆ†: {mia_auc:.3f}")
                else:
                    print(f"MIA AUC è¯„åˆ†: {mia_auc if mia_auc is not None else 'N/A'}")
            
            if 'exact_matches' in dimension_results:
                exact = dimension_results['exact_matches']
                print(f"ç²¾ç¡®åŒ¹é…é£é™©: {exact.get('risk_level', 'N/A')}")
                
                exact_percentage = exact.get('exact_match_percentage', None)
                if isinstance(exact_percentage, (int, float)):
                    print(f"ç²¾ç¡®åŒ¹é…ç™¾åˆ†æ¯”: {exact_percentage:.2f}%")
            
            if 'anonymeter' in dimension_results:
                anonymeter = dimension_results['anonymeter']
                if 'overall_risk' in anonymeter:
                    overall_risk = anonymeter['overall_risk']
                    risk_score = overall_risk.get('risk_score', None)
                    risk_level = overall_risk.get('risk_level', 'N/A')
                    if isinstance(risk_score, (int, float)):
                        print(f"Anonymeter æ€»ä½“é£é™©è¯„åˆ†: {risk_score:.3f}")
                    print(f"Anonymeter é£é™©ç­‰çº§: {risk_level}")

        if 'correlation' in results:
            print(f"\nğŸ”— è·¨æ¨¡æ€ç›¸å…³æ€§")
            print("-" * 30)
            for k, v in results['correlation'].items():
                print(f"{k}: {v}")

if __name__ == "__main__":
    main() 