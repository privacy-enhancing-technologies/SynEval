#!/usr/bin/env python3

import argparse
import pandas as pd
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
# ç§»é™¤æœ‰é—®é¢˜çš„å¯¼å…¥
# from fidelity import FidelityEvaluator
# from utility import UtilityEvaluator
# from diversity import DiversityEvaluator
# from privacy import PrivacyEvaluator
import logging
from textblob import TextBlob


class SynEvalDataFilter:
    def __init__(self, data: pd.DataFrame, metadata: Dict):
        """
        åŸºäºSynEvalè¯„ä¼°æ¡†æ¶çš„æ•°æ®ç­›é€‰å™¨
        
        Args:
            data: è¦ç­›é€‰çš„æ•°æ®é›†
            metadata: æ•°æ®é›†çš„å…ƒæ•°æ®ä¿¡æ¯
        """
        self.data = data.copy()
        self.metadata = metadata
        self.logger = logging.getLogger(__name__)
        
        # ç§»é™¤è¯„ä¼°å™¨åˆå§‹åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦å®é™…çš„è¯„ä¼°å™¨
        # self._fidelity_evaluator = None
        # self._diversity_evaluator = None
        # self._privacy_evaluator = None
        # self._utility_evaluator = None

    def calculate_text_length_scores(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—æ–‡æœ¬é•¿åº¦å¾—åˆ†ï¼ˆfidelityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œæ–‡æœ¬é•¿åº¦å¾—åˆ†å°†ä¸º0")
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                # è®¡ç®—æ–‡æœ¬é•¿åº¦
                text_lengths = self.data[col].astype(str).str.len()
                self.logger.debug(f"åˆ— {col} çš„æ–‡æœ¬é•¿åº¦èŒƒå›´: {text_lengths.min()} - {text_lengths.max()}")
                
                # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
                if text_lengths.max() > text_lengths.min():
                    normalized_lengths = (text_lengths - text_lengths.min()) / (text_lengths.max() - text_lengths.min())
                else:
                    normalized_lengths = pd.Series(0.5, index=text_lengths.index)
                scores += normalized_lengths
            else:
                self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        return scores / len(text_columns) if text_columns else scores

    def calculate_sentiment_rating_alignment_scores(self, text_columns: List[str], rating_column: str) -> pd.Series:
        """è®¡ç®—æƒ…æ„Ÿä¸è¯„åˆ†çš„åŒ¹é…åº¦å¾—åˆ†ï¼ˆutilityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if rating_column not in self.data.columns:
            self.logger.warning(f"è¯„åˆ†åˆ— {rating_column} ä¸å­˜åœ¨ï¼Œè·³è¿‡æƒ…æ„Ÿè¯„åˆ†åŒ¹é…è®¡ç®—")
            return scores
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œæƒ…æ„Ÿè¯„åˆ†åŒ¹é…åº¦å¾—åˆ†å°†ä¸º0")
            return scores
        
        try:
            for col in text_columns:
                if col in self.data.columns:
                    texts = self.data[col].astype(str)
                    ratings = pd.to_numeric(self.data[rating_column], errors='coerce').fillna(3.0)
                    
                    # è®¡ç®—æƒ…æ„Ÿææ€§
                    sentiment_scores = texts.apply(lambda x: TextBlob(x).sentiment.polarity)
                    
                    # å°†æƒ…æ„Ÿææ€§è½¬æ¢ä¸ºæœŸæœ›çš„è¯„åˆ†èŒƒå›´
                    # è´Ÿé¢æƒ…æ„Ÿ(-1åˆ°0) -> æœŸæœ›è¯„åˆ†1-3
                    # ä¸­æ€§æƒ…æ„Ÿ(0) -> æœŸæœ›è¯„åˆ†3
                    # æ­£é¢æƒ…æ„Ÿ(0åˆ°1) -> æœŸæœ›è¯„åˆ†3-5
                    expected_ratings = 3.0 + sentiment_scores * 2.0
                    
                    # è®¡ç®—å®é™…è¯„åˆ†ä¸æœŸæœ›è¯„åˆ†çš„åŒ¹é…åº¦
                    # ä½¿ç”¨è´Ÿçš„ç»å¯¹å·®å€¼ï¼Œå·®å€¼è¶Šå°å¾—åˆ†è¶Šé«˜
                    alignment_scores = 1.0 - abs(ratings - expected_ratings) / 4.0  # 4.0æ˜¯æœ€å¤§å¯èƒ½å·®å€¼
                    alignment_scores = alignment_scores.clip(0.0, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´
                    
                    self.logger.debug(f"åˆ— {col} çš„æƒ…æ„Ÿè¯„åˆ†åŒ¹é…åº¦èŒƒå›´: {alignment_scores.min():.3f} - {alignment_scores.max():.3f}")
                    scores += alignment_scores
                else:
                    self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
            
            return scores / len(text_columns) if text_columns else scores
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—æƒ…æ„Ÿè¯„åˆ†åŒ¹é…åº¦æ—¶å‡ºé”™: {e}")
            return pd.Series(0.5, index=self.data.index)

    def calculate_categorical_distribution_similarity(self, categorical_columns: List[str]) -> pd.Series:
        """è®¡ç®—åˆ†ç±»æ•°æ®çš„åˆ†å¸ƒç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆdiversityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not categorical_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šåˆ†ç±»åˆ—ï¼Œåˆ†ç±»åˆ†å¸ƒç›¸ä¼¼æ€§å¾—åˆ†å°†ä¸º0")
            return scores
        
        for col in categorical_columns:
            if col in self.data.columns:
                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„é¢‘ç‡
                value_counts = self.data[col].value_counts()
                total_count = len(self.data)
                
                # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºç¨€æœ‰åº¦ï¼Œç¨€æœ‰ç±»åˆ«å¾—åˆ†æ›´é«˜ï¼‰
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è®¡ç®—ä¸ç†æƒ³åˆ†å¸ƒçš„ç›¸ä¼¼æ€§
                rarity_scores = self.data[col].map(lambda x: 1 - (value_counts.get(x, 0) / total_count))
                scores += rarity_scores.fillna(0)
                
                self.logger.debug(f"åˆ— {col} çš„åˆ†ç±»åˆ†å¸ƒç›¸ä¼¼æ€§èŒƒå›´: {rarity_scores.min():.3f} - {rarity_scores.max():.3f}")
            else:
                self.logger.warning(f"åˆ†ç±»åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        return scores / len(categorical_columns) if categorical_columns else scores

    def calculate_semantic_diversity_scores(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—è¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†ï¼ˆdiversityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œè¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†å°†ä¸º0")
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                texts = self.data[col].astype(str)
                
                # ç®€åŒ–çš„è¯­ä¹‰å¤šæ ·æ€§è®¡ç®—
                # 1. è¯æ±‡ä¸°å¯Œåº¦
                unique_words = texts.str.split().apply(lambda x: len(set(x)) if x else 0)
                total_words = texts.str.split().apply(lambda x: len(x) if x else 1)
                vocabulary_richness = unique_words / total_words
                
                # 2. å¥å­å¤æ‚åº¦
                sentences = texts.str.split('[.!?]')
                sentence_count = sentences.apply(lambda x: len([s for s in x if s.strip()]) if x else 1)
                avg_sentence_length = texts.str.len() / sentence_count
                
                # 3. ç»¼åˆè¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†
                semantic_scores = (vocabulary_richness + avg_sentence_length / avg_sentence_length.max()) / 2
                scores += semantic_scores.fillna(0.5)
                
                self.logger.debug(f"åˆ— {col} çš„è¯­ä¹‰å¤šæ ·æ€§èŒƒå›´: {semantic_scores.min():.3f} - {semantic_scores.max():.3f}")
            else:
                self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        return scores / len(text_columns) if text_columns else scores

    def calculate_named_entities_scores(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—å‘½åå®ä½“å¾—åˆ†ï¼ˆprivacyç»´åº¦ï¼Œå®ä½“è¶Šå°‘å¾—åˆ†è¶Šé«˜ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œå‘½åå®ä½“å¾—åˆ†å°†ä¸º0")
            return scores
        
        try:
            from flair.models import SequenceTagger
            from flair.data import Sentence
            
            # åˆå§‹åŒ–NERæ¨¡å‹
            tagger = SequenceTagger.load('ner')
            
            for col in text_columns:
                if col in self.data.columns:
                    texts = self.data[col].astype(str)
                    
                    # è®¡ç®—æ¯ä¸ªæ–‡æœ¬çš„å®ä½“æ•°é‡
                    entity_counts = []
                    for text in texts:
                        if pd.isna(text) or text.strip() == '':
                            entity_counts.append(0)
                        else:
                            sentence = Sentence(text)
                            tagger.predict(sentence)
                            entity_count = len(sentence.get_spans('ner'))
                            entity_counts.append(entity_count)
                    
                    # è½¬æ¢ä¸ºå¾—åˆ†ï¼ˆå®ä½“è¶Šå°‘å¾—åˆ†è¶Šé«˜ï¼‰
                    entity_counts = pd.Series(entity_counts)
                    if entity_counts.max() > 0:
                        # æ ‡å‡†åŒ–ï¼šå®ä½“æ•°é‡è¶Šå°‘ï¼Œå¾—åˆ†è¶Šé«˜
                        entity_scores = 1.0 - (entity_counts / entity_counts.max())
                    else:
                        entity_scores = pd.Series(1.0, index=entity_counts.index)
                    
                    scores += entity_scores
                    self.logger.debug(f"åˆ— {col} çš„å‘½åå®ä½“å¾—åˆ†èŒƒå›´: {entity_scores.min():.3f} - {entity_scores.max():.3f}")
                else:
                    self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
            
            return scores / len(text_columns) if text_columns else scores
            
        except Exception as e:
            self.logger.warning(f"NERæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è®¡ç®—: {e}")
            # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ–‡æœ¬é•¿åº¦å’Œç‰¹æ®Šå­—ç¬¦æ•°é‡ä¼°ç®—
            for col in text_columns:
                if col in self.data.columns:
                    texts = self.data[col].astype(str)
                    
                    # åŸºäºæ–‡æœ¬ç‰¹å¾ä¼°ç®—å®ä½“æ•°é‡
                    # åŒ…å«æ•°å­—ã€å¤§å†™å­—æ¯ã€ç‰¹æ®Šç¬¦å·çš„æ–‡æœ¬å¯èƒ½åŒ…å«æ›´å¤šå®ä½“
                    has_numbers = texts.str.contains(r'\d')
                    has_uppercase = texts.str.contains(r'[A-Z]')
                    has_special_chars = texts.str.contains(r'[^\w\s]')
                    
                    # ç»¼åˆç‰¹å¾å¾—åˆ†ï¼ˆç‰¹å¾è¶Šå°‘ï¼Œå®ä½“å¯èƒ½è¶Šå°‘ï¼‰
                    feature_scores = 1.0 - (has_numbers.astype(int) + has_uppercase.astype(int) + has_special_chars.astype(int)) / 3.0
                    scores += feature_scores
                    
                    self.logger.debug(f"åˆ— {col} çš„ç®€åŒ–å‘½åå®ä½“å¾—åˆ†èŒƒå›´: {feature_scores.min():.3f} - {feature_scores.max():.3f}")
                else:
                    self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
            
            return scores / len(text_columns) if text_columns else scores

    def get_column_types(self) -> Dict[str, List[str]]:
        """è·å–æŒ‰ç±»å‹åˆ†ç»„çš„åˆ—å"""
        column_types = {
            'text': [],
            'numerical': [],
            'categorical': [],
            'pii': []
        }
        
        for col, info in self.metadata.get('columns', {}).items():
            if col in self.data.columns:
                col_type = info.get('sdtype', 'unknown')
                if col_type in column_types:
                    column_types[col_type].append(col)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºPII
                if info.get('pii', False):
                    column_types['pii'].append(col)
        
        self.logger.info(f"æ£€æµ‹åˆ°çš„åˆ—ç±»å‹: {column_types}")
        return column_types

    def get_metadata_config(self) -> Dict:
        """ä»metadataä¸­è·å–é…ç½®ä¿¡æ¯"""
        config = {}
        
        # è·å–æ–‡æœ¬åˆ—é…ç½®
        if 'text_columns' in self.metadata:
            config['text_columns'] = self.metadata['text_columns']
            self.logger.info(f"ä»metadataè·å–æ–‡æœ¬åˆ—: {config['text_columns']}")
        
        # è·å–utilityé…ç½®
        if 'utility' in self.metadata:
            utility_config = self.metadata['utility']
            if 'input_columns' in utility_config and 'output_columns' in utility_config:
                config['utility_input'] = utility_config['input_columns']
                config['utility_output'] = utility_config['output_columns']
                self.logger.info(f"ä»metadataè·å–utilityé…ç½®: è¾“å…¥={config['utility_input']}, è¾“å‡º={config['utility_output']}")
        
        return config

    def calculate_syn_eval_scores(self, config: Dict) -> pd.DataFrame:
        """
        è®¡ç®—åŸºäºSynEvalæ¡†æ¶çš„å¾—åˆ†
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«å„ç»´åº¦çš„å‚æ•°
        
        Returns:
            åŒ…å«æ‰€æœ‰å¾—åˆ†çš„DataFrame
        """
        scores_df = pd.DataFrame(index=self.data.index)
        column_types = self.get_column_types()
        metadata_config = self.get_metadata_config()
        
        # è·å–åˆ—é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨metadataä¸­çš„é…ç½®ï¼Œç„¶åæ˜¯æŒ‡å®šçš„é…ç½®ï¼Œæœ€åæ˜¯è‡ªåŠ¨æ£€æµ‹
        text_cols = config.get('text_columns') or metadata_config.get('text_columns') or column_types['text']
        rating_col = config.get('rating_column', 'rating')
        cat_cols = config.get('categorical_columns') or column_types['categorical']
        
        # å¯¹äºutilityï¼Œä½¿ç”¨metadataä¸­çš„é…ç½®
        utility_input_cols = metadata_config.get('utility_input', text_cols)
        utility_output_cols = metadata_config.get('utility_output', [rating_col])
        
        self.logger.info(f"ä½¿ç”¨çš„æ–‡æœ¬åˆ—: {text_cols}")
        self.logger.info(f"ä½¿ç”¨çš„è¯„åˆ†åˆ—: {rating_col}")
        self.logger.info(f"ä½¿ç”¨çš„åˆ†ç±»åˆ—: {cat_cols}")
        self.logger.info(f"Utilityè¾“å…¥åˆ—: {utility_input_cols}")
        self.logger.info(f"Utilityè¾“å‡ºåˆ—: {utility_output_cols}")
        
        # 1. Fidelityç»´åº¦ (25%æƒé‡)
        # 1.1 æ–‡æœ¬é•¿åº¦ (fidelityçš„1/3æƒé‡)
        text_length_scores = self.calculate_text_length_scores(text_cols)
        self.logger.info(f"æ–‡æœ¬é•¿åº¦å¾—åˆ†èŒƒå›´: {text_length_scores.min():.3f} - {text_length_scores.max():.3f}")
        
        # 1.2 Diagnostic overall score (fidelityçš„1/3æƒé‡)
        text_quality_scores = self.calculate_text_quality_proxy(text_cols)
        self.logger.info(f"è¯Šæ–­å¾—åˆ†èŒƒå›´: {text_quality_scores.min():.3f} - {text_quality_scores.max():.3f}")
        
        # 1.3 Quality overall score (fidelityçš„1/3æƒé‡)
        complexity_scores = self.calculate_text_complexity_proxy(text_cols)
        self.logger.info(f"è´¨é‡å¾—åˆ†èŒƒå›´: {complexity_scores.min():.3f} - {complexity_scores.max():.3f}")
        
        # è®¡ç®—Fidelityæ€»åˆ† (ä¸‰ä¸ªå­æŒ‡æ ‡çš„å¹³å‡å€¼)
        fidelity_scores = (text_length_scores + text_quality_scores + complexity_scores) / 3
        scores_df['fidelity_score'] = fidelity_scores
        self.logger.info(f"Fidelityæ€»åˆ†èŒƒå›´: {fidelity_scores.min():.3f} - {fidelity_scores.max():.3f}")
        
        # 2. Utilityç»´åº¦ (25%æƒé‡)
        # 2.1 æƒ…æ„Ÿä¸è¯„åˆ†åŒ¹é…åº¦ - ä½¿ç”¨metadataä¸­çš„utilityé…ç½®
        if utility_input_cols and utility_output_cols:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å…¥åˆ—å’Œç¬¬ä¸€ä¸ªè¾“å‡ºåˆ—
            input_col = utility_input_cols[0] if utility_input_cols else text_cols[0]
            output_col = utility_output_cols[0] if utility_output_cols else rating_col
            sentiment_alignment_scores = self.calculate_sentiment_rating_alignment_scores([input_col], output_col)
        else:
            sentiment_alignment_scores = self.calculate_sentiment_rating_alignment_scores(text_cols, rating_col)
        
        scores_df['utility_score'] = sentiment_alignment_scores
        self.logger.info(f"Utilityå¾—åˆ†èŒƒå›´: {sentiment_alignment_scores.min():.3f} - {sentiment_alignment_scores.max():.3f}")
        
        # 3. Diversityç»´åº¦ (25%æƒé‡)
        # 3.1 åˆ†ç±»åˆ†å¸ƒç›¸ä¼¼æ€§ (diversityçš„1/2æƒé‡)
        cat_dist_scores = self.calculate_categorical_distribution_similarity(cat_cols)
        self.logger.info(f"åˆ†ç±»åˆ†å¸ƒç›¸ä¼¼æ€§èŒƒå›´: {cat_dist_scores.min():.3f} - {cat_dist_scores.max():.3f}")
        
        # 3.2 è¯­ä¹‰å¤šæ ·æ€§ (diversityçš„1/2æƒé‡)
        semantic_scores = self.calculate_semantic_diversity_scores(text_cols)
        self.logger.info(f"è¯­ä¹‰å¤šæ ·æ€§èŒƒå›´: {semantic_scores.min():.3f} - {semantic_scores.max():.3f}")
        
        # è®¡ç®—Diversityæ€»åˆ† (ä¸¤ä¸ªå­æŒ‡æ ‡çš„å¹³å‡å€¼)
        diversity_scores = (cat_dist_scores + semantic_scores) / 2
        scores_df['diversity_score'] = diversity_scores
        self.logger.info(f"Diversityæ€»åˆ†èŒƒå›´: {diversity_scores.min():.3f} - {diversity_scores.max():.3f}")
        
        # 4. Privacyç»´åº¦ (25%æƒé‡)
        # 4.1 å‘½åå®ä½“æ•°é‡
        named_entities_scores = self.calculate_named_entities_scores(text_cols)
        scores_df['privacy_score'] = named_entities_scores
        self.logger.info(f"Privacyå¾—åˆ†èŒƒå›´: {named_entities_scores.min():.3f} - {named_entities_scores.max():.3f}")
        
        return scores_df

    def calculate_text_quality_proxy(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—æ–‡æœ¬è´¨é‡ä»£ç†å¾—åˆ†ï¼ˆç”¨äºdiagnosticï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                texts = self.data[col].astype(str)
                
                # æ–‡æœ¬è´¨é‡æŒ‡æ ‡
                lengths = texts.str.len()
                length_scores = 1 - abs(lengths - lengths.median()) / lengths.max()
                
                unique_words = texts.str.split().apply(lambda x: len(set(x)) if x else 0)
                total_words = texts.str.split().apply(lambda x: len(x) if x else 1)
                vocabulary_scores = unique_words / total_words
                
                punctuation_count = texts.str.count(r'[.!?,;:]')
                punctuation_scores = 1 - abs(punctuation_count - punctuation_count.median()) / (punctuation_count.max() + 1)
                
                col_scores = (length_scores + vocabulary_scores + punctuation_scores) / 3
                scores += col_scores.fillna(0)
        
        return scores / len(text_columns) if text_columns else scores

    def calculate_text_complexity_proxy(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—æ–‡æœ¬å¤æ‚åº¦ä»£ç†å¾—åˆ†ï¼ˆç”¨äºqualityï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                texts = self.data[col].astype(str)
                
                # å¥å­é•¿åº¦
                sentences = texts.str.split('[.!?]')
                avg_sentence_length = sentences.apply(lambda x: np.mean([len(s.split()) for s in x if s.strip()]) if x else 0)
                
                # è¯æ±‡å¤æ‚åº¦
                words = texts.str.split()
                long_words = words.apply(lambda x: sum(1 for w in x if len(w) > 6) if x else 0)
                total_words = words.apply(lambda x: len(x) if x else 1)
                complexity_ratio = long_words / total_words
                
                # æ ‡å‡†åŒ–å¾—åˆ†
                if avg_sentence_length.max() > 0:
                    sentence_scores = avg_sentence_length / avg_sentence_length.max()
                else:
                    sentence_scores = pd.Series(0.5, index=avg_sentence_length.index)
                
                col_scores = (sentence_scores + complexity_ratio) / 2
                scores += col_scores.fillna(0.5)
        
        return scores / len(text_columns) if text_columns else scores

    def filter_top_data(self, config: Dict, top_n: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ç­›é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å‰Næ¡æ•°æ®
        
        Args:
            config: é…ç½®å­—å…¸
            top_n: è¦ç­›é€‰çš„æ•°æ®æ¡æ•°
        
        Returns:
            (ç­›é€‰åçš„æ•°æ®, å¾—åˆ†DataFrame)
        """
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡å¾—åˆ†
        scores_df = self.calculate_syn_eval_scores(config)
        
        # è®¡ç®—åŠ æƒæ€»åˆ† (æŒ‰ç…§æ‚¨çš„è¦æ±‚ï¼šæ¯ä¸ªå¤§ç±»25%æƒé‡)
        total_score = (
            0.25 * scores_df['fidelity_score'] +
            0.25 * scores_df['utility_score'] +
            0.25 * scores_df['diversity_score'] +
            0.25 * scores_df['privacy_score']
        )
        
        self.logger.info(f"åŠ æƒæ€»åˆ†èŒƒå›´: {total_score.min():.3f} - {total_score.max():.3f}")
        
        scores_df['total_score'] = total_score
        
        # æŒ‰æ€»åˆ†æ’åºå¹¶é€‰æ‹©å‰Næ¡
        top_indices = total_score.nlargest(top_n).index
        filtered_data = self.data.loc[top_indices].copy()
        filtered_scores = scores_df.loc[top_indices].copy()
        
        return filtered_data, filtered_scores


def parse_args():
    parser = argparse.ArgumentParser(description='åŸºäºSynEvalæ¡†æ¶çš„æ•°æ®ç­›é€‰å·¥å…·')

    parser.add_argument('--data', required=True, help='è¦ç­›é€‰çš„æ•°æ®é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--metadata', required=True, help='å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='ç­›é€‰åæ•°æ®çš„è¾“å‡ºè·¯å¾„')
    parser.add_argument('--scores-output', help='å¾—åˆ†æ•°æ®çš„è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--top-n', type=int, default=50, help='è¦ç­›é€‰çš„æ•°æ®æ¡æ•°ï¼ˆé»˜è®¤: 50ï¼‰')

    # åˆ—é…ç½®
    parser.add_argument('--text-columns', nargs='+', help='æ–‡æœ¬åˆ—åï¼ˆå¦‚reviewï¼‰')
    parser.add_argument('--rating-column', default='rating', help='è¯„åˆ†åˆ—åï¼ˆé»˜è®¤: ratingï¼‰')
    parser.add_argument('--categorical-columns', nargs='+', help='åˆ†ç±»åˆ—åï¼ˆå¦‚categoryï¼‰')

    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], help='è®¾ç½®æ—¥å¿—çº§åˆ«')
    
    return parser

def main():
    parser = parse_args()
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # åŠ è½½æ•°æ®
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.data}")
    data = pd.read_csv(args.data)

    logger.info(f"æ­£åœ¨åŠ è½½å…ƒæ•°æ®: {args.metadata}")
    with open(args.metadata, 'r') as f:
        metadata = json.load(f)

    # åˆ é™¤æœªåœ¨å…ƒæ•°æ®ä¸­å®šä¹‰çš„ _id
    metadata_columns = metadata.get("columns", {})
    if '_id' in data.columns and '_id' not in metadata_columns:
        logger.warning("åœ¨æ•°æ®ä¸­å‘ç° '_id' åˆ—ä½†å…ƒæ•°æ®ä¸­æœªå®šä¹‰ â€” æ­£åœ¨åˆ é™¤")
        data.drop(columns=['_id'], inplace=True)

    # æ„å»ºé…ç½®
    config = {
        'text_columns': args.text_columns,
        'rating_column': args.rating_column,
        'categorical_columns': args.categorical_columns
    }

    # åˆ›å»ºæ•°æ®ç­›é€‰å™¨
    logger.info("æ­£åœ¨åˆ›å»ºSynEvalæ•°æ®ç­›é€‰å™¨...")
    data_filter = SynEvalDataFilter(data, metadata)

    # ç­›é€‰æ•°æ®
    logger.info(f"æ­£åœ¨ç­›é€‰å‰{args.top_n}æ¡æ•°æ®...")
    filtered_data, filtered_scores = data_filter.filter_top_data(config, top_n=args.top_n)

    # ä¿å­˜ç»“æœ
    logger.info(f"æ­£åœ¨ä¿å­˜ç­›é€‰åçš„æ•°æ®åˆ°: {args.output}")
    filtered_data.to_csv(args.output, index=False)
    
    if args.scores_output:
        logger.info(f"æ­£åœ¨ä¿å­˜å¾—åˆ†æ•°æ®åˆ°: {args.scores_output}")
        filtered_scores.to_csv(args.scores_output, index=False)

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š SynEvalæ•°æ®ç­›é€‰ç»“æœæ‘˜è¦")
    print("=" * 50)
    print(f"åŸå§‹æ•°æ®æ¡æ•°: {len(data)}")
    print(f"ç­›é€‰åæ•°æ®æ¡æ•°: {len(filtered_data)}")
    
    print(f"\næƒé‡é…ç½®:")
    print(f"  Fidelityç»´åº¦: 25% (æ–‡æœ¬é•¿åº¦ã€è¯Šæ–­å¾—åˆ†ã€è´¨é‡å¾—åˆ†å„å 8.33%)")
    print(f"  Utilityç»´åº¦: 25% (æƒ…æ„Ÿè¯„åˆ†åŒ¹é…åº¦)")
    print(f"  Diversityç»´åº¦: 25% (åˆ†ç±»åˆ†å¸ƒç›¸ä¼¼æ€§ã€è¯­ä¹‰å¤šæ ·æ€§å„å 12.5%)")
    print(f"  Privacyç»´åº¦: 25% (å‘½åå®ä½“æ•°é‡)")
    
    print(f"\nç­›é€‰åæ•°æ®çš„åŸºæœ¬ç»Ÿè®¡:")
    print(f"å¹³å‡æ€»åˆ†: {filtered_scores['total_score'].mean():.3f}")
    print(f"æœ€é«˜æ€»åˆ†: {filtered_scores['total_score'].max():.3f}")
    print(f"æœ€ä½æ€»åˆ†: {filtered_scores['total_score'].min():.3f}")
    
    # æ˜¾ç¤ºå„ç»´åº¦çš„å¾—åˆ†ç»Ÿè®¡
    dimension_scores = {
        'Fidelity': ['score_text_length', 'score_diagnostic', 'score_quality'],
        'Utility': ['score_sentiment_alignment'],
        'Diversity': ['score_categorical_distribution', 'score_semantic_diversity'],
        'Privacy': ['score_named_entities']
    }
    
    for dimension, score_cols in dimension_scores.items():
        available_cols = [col for col in score_cols if col in filtered_scores.columns]
        if available_cols:
            avg_score = filtered_scores[available_cols].sum(axis=1).mean()
            print(f"{dimension} å¹³å‡å¾—åˆ†: {avg_score:.3f}")

    logger.info("æ•°æ®ç­›é€‰å®Œæˆ!")

if __name__ == "__main__":
    main() 