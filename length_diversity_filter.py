#!/usr/bin/env python3

import argparse
import pandas as pd
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from textblob import TextBlob


class LengthDiversityFilter:
    def __init__(self, data: pd.DataFrame, metadata: Dict):
        """
        åŸºäºæ–‡æœ¬é•¿åº¦å’Œè¯­ä¹‰å¤šæ ·æ€§çš„æ•°æ®ç­›é€‰å™¨
        
        Args:
            data: è¦ç­›é€‰çš„æ•°æ®é›†
            metadata: æ•°æ®é›†çš„å…ƒæ•°æ®ä¿¡æ¯
        """
        self.data = data.copy()
        self.metadata = metadata
        self.logger = logging.getLogger(__name__)

    def calculate_text_length_scores(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—æ–‡æœ¬é•¿åº¦å¾—åˆ†ï¼ˆFidelityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œæ–‡æœ¬é•¿åº¦å¾—åˆ†å°†ä¸º0")
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                # è®¡ç®—æ–‡æœ¬é•¿åº¦
                text_lengths = self.data[col].astype(str).str.len()
                self.logger.debug(f"åˆ— {col} çš„æ–‡æœ¬é•¿åº¦èŒƒå›´: {text_lengths.min()} - {text_lengths.max()}")
                
                # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
                if text_lengths.max() > text_lengths.min():
                    normalized_lengths = (text_lengths - text_lengths.min()) / (text_lengths.max() - text_lengths.min())
                else:
                    normalized_lengths = pd.Series(0.5, index=text_lengths.index)
                scores += normalized_lengths
            else:
                self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        return scores / len(text_columns) if text_columns else scores

    def calculate_semantic_diversity_scores(self, text_columns: List[str]) -> pd.Series:
        """è®¡ç®—è¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†ï¼ˆDiversityç»´åº¦ï¼‰"""
        scores = pd.Series(0.0, index=self.data.index)
        
        if not text_columns:
            self.logger.warning("æ²¡æœ‰æŒ‡å®šæ–‡æœ¬åˆ—ï¼Œè¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†å°†ä¸º0")
            return scores
        
        for col in text_columns:
            if col in self.data.columns:
                texts = self.data[col].astype(str)
                
                # è¯­ä¹‰å¤šæ ·æ€§è®¡ç®—
                # 1. è¯æ±‡ä¸°å¯Œåº¦ (Type-Token Ratio)
                unique_words = texts.str.split().apply(lambda x: len(set(x)) if x else 0)
                total_words = texts.str.split().apply(lambda x: len(x) if x else 1)
                vocabulary_richness = unique_words / total_words
                
                # 2. å¥å­å¤æ‚åº¦
                sentences = texts.str.split('[.!?]')
                sentence_count = sentences.apply(lambda x: len([s for s in x if s.strip()]) if x else 1)
                avg_sentence_length = texts.str.len() / sentence_count
                
                # 3. è¯æ±‡å¤æ‚åº¦ (é•¿è¯æ¯”ä¾‹)
                words = texts.str.split()
                long_words = words.apply(lambda x: sum(1 for w in x if len(w) > 6) if x else 0)
                total_words_for_complexity = words.apply(lambda x: len(x) if x else 1)
                complexity_ratio = long_words / total_words_for_complexity
                
                # 4. æ ‡ç‚¹ç¬¦å·å¤šæ ·æ€§
                punctuation_variety = texts.str.count(r'[.!?,;:()]')
                
                # 5. ç»¼åˆè¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†
                if avg_sentence_length.max() > 0:
                    sentence_scores = avg_sentence_length / avg_sentence_length.max()
                else:
                    sentence_scores = pd.Series(0.5, index=avg_sentence_length.index)
                
                if punctuation_variety.max() > 0:
                    punctuation_scores = punctuation_variety / punctuation_variety.max()
                else:
                    punctuation_scores = pd.Series(0.5, index=punctuation_variety.index)
                
                semantic_scores = (
                    vocabulary_richness * 0.3 +  # è¯æ±‡ä¸°å¯Œåº¦æƒé‡30%
                    sentence_scores * 0.3 +      # å¥å­å¤æ‚åº¦æƒé‡30%
                    complexity_ratio * 0.2 +     # è¯æ±‡å¤æ‚åº¦æƒé‡20%
                    punctuation_scores * 0.2     # æ ‡ç‚¹å¤šæ ·æ€§æƒé‡20%
                )
                
                scores += semantic_scores.fillna(0.5)
                
                self.logger.debug(f"åˆ— {col} çš„è¯­ä¹‰å¤šæ ·æ€§èŒƒå›´: {semantic_scores.min():.3f} - {semantic_scores.max():.3f}")
            else:
                self.logger.warning(f"æ–‡æœ¬åˆ— {col} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        return scores / len(text_columns) if text_columns else scores

    def get_column_types(self) -> Dict[str, List[str]]:
        """è·å–æŒ‰ç±»å‹åˆ†ç»„çš„åˆ—å"""
        column_types = {
            'text': [],
            'numerical': [],
            'categorical': [],
            'pii': []
        }
        
        for col, info in self.metadata.get('columns', {}).items():
            if col in self.data.columns:
                col_type = info.get('sdtype', 'unknown')
                if col_type in column_types:
                    column_types[col_type].append(col)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºPII
                if info.get('pii', False):
                    column_types['pii'].append(col)
        
        self.logger.info(f"æ£€æµ‹åˆ°çš„åˆ—ç±»å‹: {column_types}")
        return column_types

    def get_metadata_config(self) -> Dict:
        """ä»metadataä¸­è·å–é…ç½®ä¿¡æ¯"""
        config = {}
        
        # è·å–æ–‡æœ¬åˆ—é…ç½®
        if 'text_columns' in self.metadata:
            config['text_columns'] = self.metadata['text_columns']
            self.logger.info(f"ä»metadataè·å–æ–‡æœ¬åˆ—: {config['text_columns']}")
        
        return config

    def calculate_scores(self, config: Dict) -> pd.DataFrame:
        """
        è®¡ç®—åŸºäºæ–‡æœ¬é•¿åº¦å’Œè¯­ä¹‰å¤šæ ·æ€§çš„å¾—åˆ†
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«å„ç»´åº¦çš„å‚æ•°
        
        Returns:
            åŒ…å«æ‰€æœ‰å¾—åˆ†çš„DataFrame
        """
        scores_df = pd.DataFrame(index=self.data.index)
        column_types = self.get_column_types()
        metadata_config = self.get_metadata_config()
        
        # è·å–æ–‡æœ¬åˆ—é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨metadataä¸­çš„é…ç½®ï¼Œç„¶åæ˜¯æŒ‡å®šçš„é…ç½®ï¼Œæœ€åæ˜¯è‡ªåŠ¨æ£€æµ‹
        text_cols = config.get('text_columns') or metadata_config.get('text_columns') or column_types['text']
        
        self.logger.info(f"ä½¿ç”¨çš„æ–‡æœ¬åˆ—: {text_cols}")
        
        # 1. Fidelityç»´åº¦ (50%æƒé‡) - æ–‡æœ¬é•¿åº¦
        text_length_scores = self.calculate_text_length_scores(text_cols)
        self.logger.info(f"æ–‡æœ¬é•¿åº¦å¾—åˆ†èŒƒå›´: {text_length_scores.min():.3f} - {text_length_scores.max():.3f}")
        scores_df['fidelity_score'] = text_length_scores
        
        # 2. Diversityç»´åº¦ (50%æƒé‡) - è¯­ä¹‰å¤šæ ·æ€§
        semantic_scores = self.calculate_semantic_diversity_scores(text_cols)
        self.logger.info(f"è¯­ä¹‰å¤šæ ·æ€§å¾—åˆ†èŒƒå›´: {semantic_scores.min():.3f} - {semantic_scores.max():.3f}")
        scores_df['diversity_score'] = semantic_scores
        
        return scores_df

    def filter_top_data(self, config: Dict, top_n: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ç­›é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å‰Næ¡æ•°æ®
        
        Args:
            config: é…ç½®å­—å…¸
            top_n: è¦ç­›é€‰çš„æ•°æ®æ¡æ•°
        
        Returns:
            (ç­›é€‰åçš„æ•°æ®, å¾—åˆ†DataFrame)
        """
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡å¾—åˆ†
        scores_df = self.calculate_scores(config)
        
        # è®¡ç®—åŠ æƒæ€»åˆ† (Fidelity 50%, Diversity 50%)
        total_score = (
            0.50 * scores_df['fidelity_score'] +
            0.50 * scores_df['diversity_score']
        )
        
        self.logger.info(f"åŠ æƒæ€»åˆ†èŒƒå›´: {total_score.min():.3f} - {total_score.max():.3f}")
        
        scores_df['total_score'] = total_score
        
        # æŒ‰æ€»åˆ†æ’åºå¹¶é€‰æ‹©å‰Næ¡
        top_indices = total_score.nlargest(top_n).index
        filtered_data = self.data.loc[top_indices].copy()
        filtered_scores = scores_df.loc[top_indices].copy()
        
        return filtered_data, filtered_scores


def parse_args():
    parser = argparse.ArgumentParser(description='åŸºäºæ–‡æœ¬é•¿åº¦å’Œè¯­ä¹‰å¤šæ ·æ€§çš„æ•°æ®ç­›é€‰å·¥å…·')

    parser.add_argument('--data', required=True, help='è¦ç­›é€‰çš„æ•°æ®é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--metadata', required=True, help='å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='ç­›é€‰åæ•°æ®çš„è¾“å‡ºè·¯å¾„')
    parser.add_argument('--scores-output', help='å¾—åˆ†æ•°æ®çš„è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--top-n', type=int, default=50, help='è¦ç­›é€‰çš„æ•°æ®æ¡æ•°ï¼ˆé»˜è®¤: 50ï¼‰')

    # åˆ—é…ç½®
    parser.add_argument('--text-columns', nargs='+', help='æ–‡æœ¬åˆ—åï¼ˆå¦‚title, textï¼‰')

    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], help='è®¾ç½®æ—¥å¿—çº§åˆ«')
    
    return parser

def main():
    parser = parse_args()
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # åŠ è½½æ•°æ®
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.data}")
    data = pd.read_csv(args.data)

    logger.info(f"æ­£åœ¨åŠ è½½å…ƒæ•°æ®: {args.metadata}")
    with open(args.metadata, 'r') as f:
        metadata = json.load(f)

    # åˆ é™¤æœªåœ¨å…ƒæ•°æ®ä¸­å®šä¹‰çš„ _id
    metadata_columns = metadata.get("columns", {})
    if '_id' in data.columns and '_id' not in metadata_columns:
        logger.warning("åœ¨æ•°æ®ä¸­å‘ç° '_id' åˆ—ä½†å…ƒæ•°æ®ä¸­æœªå®šä¹‰ â€” æ­£åœ¨åˆ é™¤")
        data.drop(columns=['_id'], inplace=True)

    # æ„å»ºé…ç½®
    config = {
        'text_columns': args.text_columns,
    }

    # åˆ›å»ºæ•°æ®ç­›é€‰å™¨
    logger.info("æ­£åœ¨åˆ›å»ºé•¿åº¦-å¤šæ ·æ€§æ•°æ®ç­›é€‰å™¨...")
    data_filter = LengthDiversityFilter(data, metadata)

    # ç­›é€‰æ•°æ®
    logger.info(f"æ­£åœ¨ç­›é€‰å‰{args.top_n}æ¡æ•°æ®...")
    filtered_data, filtered_scores = data_filter.filter_top_data(config, top_n=args.top_n)

    # ä¿å­˜ç»“æœ
    logger.info(f"æ­£åœ¨ä¿å­˜ç­›é€‰åçš„æ•°æ®åˆ°: {args.output}")
    filtered_data.to_csv(args.output, index=False)
    
    if args.scores_output:
        logger.info(f"æ­£åœ¨ä¿å­˜å¾—åˆ†æ•°æ®åˆ°: {args.scores_output}")
        filtered_scores.to_csv(args.scores_output, index=False)

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š é•¿åº¦-å¤šæ ·æ€§æ•°æ®ç­›é€‰ç»“æœæ‘˜è¦")
    print("=" * 50)
    print(f"åŸå§‹æ•°æ®æ¡æ•°: {len(data)}")
    print(f"ç­›é€‰åæ•°æ®æ¡æ•°: {len(filtered_data)}")
    
    print(f"\næƒé‡é…ç½®:")
    print(f"  Fidelityç»´åº¦: 50% (æ–‡æœ¬é•¿åº¦)")
    print(f"  Diversityç»´åº¦: 50% (è¯­ä¹‰å¤šæ ·æ€§)")
    print(f"  Utilityç»´åº¦: 0% (ä¸å‚ä¸è¯„åˆ†)")
    print(f"  Privacyç»´åº¦: 0% (ä¸å‚ä¸è¯„åˆ†)")
    
    print(f"\nç­›é€‰åæ•°æ®çš„åŸºæœ¬ç»Ÿè®¡:")
    print(f"å¹³å‡æ€»åˆ†: {filtered_scores['total_score'].mean():.3f}")
    print(f"æœ€é«˜æ€»åˆ†: {filtered_scores['total_score'].max():.3f}")
    print(f"æœ€ä½æ€»åˆ†: {filtered_scores['total_score'].min():.3f}")
    
    print(f"Fidelity (æ–‡æœ¬é•¿åº¦) å¹³å‡å¾—åˆ†: {filtered_scores['fidelity_score'].mean():.3f}")
    print(f"Diversity (è¯­ä¹‰å¤šæ ·æ€§) å¹³å‡å¾—åˆ†: {filtered_scores['diversity_score'].mean():.3f}")

    logger.info("æ•°æ®ç­›é€‰å®Œæˆ!")

if __name__ == "__main__":
    main() 